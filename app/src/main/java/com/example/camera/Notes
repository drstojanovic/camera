Problems and fixes:

- Black screen on starting camera (sometimes)
    In about 50% cases, after opening camera, surface view was black, but in Logcat I could see that
    camera was receiving bytes. Problem was that surface view fixed size and aspect ratio were not set
    correctly before opening camera (even call was after that, but obviously on other thread (Camera thread).
    This was resolved by POSTING rest of code on SURFACE VIEW thread. That way we are sure that all operations
    will be executed AFTER proper size is set. This was found in official Camera2Basic google example.

- Freezing camera after a while on preview mode
    Actually, exception was thrown in "cameraManager.open" method and its code was "1" which means that
    camera was in use. Beside this error, in log were also found these:
        - E/Legacy-CameraDevice-JNI: getNativeWindow: Surface had no valid native window.
        - E/Legacy-CameraDevice-JNI: LegacyCameraDevice_nativeDetectSurfaceType: Could not retrieve native window from surface
        - E/CameraDeviceGLThread-0: Received exception on GL render thread:
              java.lang.IllegalArgumentException: Surface had no valid native Surface.
    What was actually happening is that GarbageCollector was destroying ImageReader instance which was
    created in method.
    Problem was resolved by adding ImageReader as class field.
    Solution: https://stackoverflow.com/a/33468855/8177021

- Slow rendering and lagging after few seconds
    This behaviour was spotted when using SurfaceView from official Camera2Basic google example. Even when low image resolution
    is set, there was still lagging visible on my phone (not emulator). Lag was not found on official TensorFlowLite Android example.
    They were actually using default TextureView and most importantly, there was setting of default buffer size:
    textureView.surfaceTexture.setDefaultBufferSize(previewSize.width, previewSize.height)


FLOW
- set callback on SurfacePreview; when surface is created, setup camera
- get camera id by matching conditions (lens, orientation..)
- get output size (regarding selected camera and desired resolution)
- set size and aspect ratio to surface view

- open camera using camera handler
- setup image reader
- create capture session with two surface destinations (preview and image reader) with repeating request

>> ImageProcessor:
- preprocess, process, add processing info to result (duration, size etc.)
1. preprocess
   - rotate image depending on device sensor orientation
   - resize image depending on required model input size (from settings)
2. process
   - Local: get image bytes, call detector, filter invalid detections (by size of detection and count)
   - Remote: get image bytes, encode them and emmit to server via socket
3. each processor (local and remote) generates recognitionTime and imageSize. Final processing phase is to:
   - calculate average recognition time
   - calculate average image size

CAMERA IMPROVEMENTS:
- auto focus and brightness
- locking the phone and restarting the app
- start/stop threads in onResume/onPause callbacks
- check postInferenceCallback solution for isProcessing = false flag

CODE IMPROVEMENTS:
- use dependency injection framework
    - how to dynamically inject ImageProcessor?
- create MainViewModel
- TrackerOverlayView/cornerSize, why depends on width and height?
- LocalImageProcessor, add option to completely disable the compression process; compare speed with and without compression

TODO:
- app icon
- create MainViewModel
- base activity
- empty address and port handling
- fix freezing when minimizing and opening recent apps and back to app
- increase number of detections to 20?
- Remote processing:
    - internet check before start
    - slow-internet (warning message)
    - error handling (show toast)
    - reconnect option
